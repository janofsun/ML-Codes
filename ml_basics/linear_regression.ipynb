{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the **Nnormal Equation** to solve Linear Regression:\n",
    "$$\\theta = (X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression Model\n",
    "The linear regression model can be represented as: $(h_{\\theta}(x) = \\theta^T x)$ where $(h_{\\theta}(x))$ is the model's prediction, $(\\theta)$ represents the model parameters (including intercept and slope), and $(x)$ is the feature vector.\n",
    "\n",
    "##### Cost Function\n",
    "The goal of linear regression is to minimize the cost $(\\theta)$, thereby minimizing the cost function $(J(\\theta))$. The cost function is the sum of the squares of the errors between predicted and actual values, given by:\n",
    "$[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2 ]$\n",
    "Here, $(m)$ is the number of training samples, $(x^{(i)})$ is the feature vector of the $i^{th}$ sample, and $(y^{(i)})$ is the actual target value of the $i^{th}$ sample.\n",
    "\n",
    "##### Matrix Representation of the Cost Function\n",
    "Representing the cost function in matrix form simplifies computation. Define matrix $(X)$ where each row is a feature vector of a training sample (usually the first column is 1, representing the intercept), and vector $(y)$ contains all training samples' target values. The cost function can then be expressed as:\n",
    "$[ J(\\theta) = \\frac{1}{2} (X\\theta - y)^T (X\\theta - y) ]$\n",
    "\n",
    "##### Derivation of the Cost Function\n",
    "To find the $(\\theta)$ that minimizes the cost function, we derive with respect to $(\\theta)$ and set the derivative equal to zero. The derivative of $(J(\\theta))$ is given by:\n",
    "$[ \\frac{\\partial J}{\\partial \\theta} = X^T(X\\theta - y) ]$\n",
    "Setting the derivative to zero, we find:\n",
    "$[ X^T(X\\theta - y) = 0 ]$\n",
    "$[ X^TX\\theta - X^Ty = 0 ]$\n",
    "$[ X^TX\\theta = X^Ty ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.8817842e-16 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n",
    "\t# Your code here, make sure to round\n",
    "    x = np.array(X) # 3x2\n",
    "    x_trans = np.transpose(x) # 2x3\n",
    "    y = np.array(y) # 1x3\n",
    "    theta = np.linalg.inv(x_trans.dot(x)).dot(x_trans).dot(y)\n",
    "    return theta\n",
    "\n",
    "# input_x = [[1, 1], [1, 2], [1, 3]]\n",
    "# input_y = [1, 2, 3]\n",
    "# print(linear_regression_normal_equation(input_x, input_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Hypothesis Function**:\n",
    "    - The hypothesis for linear regression is given by:\n",
    "      $$[h_{\\theta}(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\ldots + \\theta_n X_n]$$\n",
    "      where $( X )$ is the input features matrix, and $( \\theta )$ are the parameters (weights).\n",
    "\n",
    "2. **Cost Function**:\n",
    "    - The cost function (MSE) measures how well the hypothesis fits the training data:\n",
    "      $$[\n",
    "      J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(X^{(i)}) - y^{(i)} \\right)^2\n",
    "      ]$$\n",
    "      where $( m )$ is the number of training examples.\n",
    "      $$\\delta J(\\theta) = \\frac{1}{m} X^{T} (X\\theta - y)$$\n",
    "\n",
    "3. **Gradient Descent Algorithm**:\n",
    "    - Gradient descent updates the parameters $( \\theta )$ iteratively:\n",
    "      $$[\n",
    "      \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(X^{(i)}) - y^{(i)} \\right) \\cdot X_j^{(i)}\n",
    "      ]$$\n",
    "      $$\\theta := \\theta - \\alpha \\cdot \\delta J(\\theta)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11071521]\n",
      " [0.95129619]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "\t# Your code here, make sure to round\n",
    "\tm, n = X.shape\n",
    "\tx_trans = np.transpose(X) # n m\n",
    "\ttheta = np.zeros((n, 1)) # n 1\n",
    "\ty_trans = y.reshape(-1, 1)\n",
    "\tfor _ in range(iterations):\n",
    "\t\tpred = X.dot(theta) # m 1\n",
    "\t\tdifference = (1/m)*x_trans.dot(pred - y_trans)\n",
    "\t\ttheta -= alpha * difference\n",
    "\treturn theta\n",
    "\n",
    "input_x = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "input_y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "print(linear_regression_gradient_descent(input_x, input_y, alpha, iterations))\n",
    "# output: np.array([0.1107, 0.9513])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use * for element-wise multiplication.\n",
    "- Use @ for matrix multiplication (preferred for readability).\n",
    "- Use np.dot for more complex operations like dot products of 1-D arrays or higher-dimensional array multiplication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
