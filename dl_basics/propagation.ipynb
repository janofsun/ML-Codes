{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Neuron\n",
    "\n",
    "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n",
    "\n",
    "- Example:\n",
    "\n",
    "        - input: features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], labels = [0, 1, 0], weights = [0.7, -0.4], bias = -0.1\n",
    "\n",
    "        - output: ([0.4626, 0.4134, 0.6682], 0.3349)\n",
    "        \n",
    "        - reasoning: For each input vector, the weighted sum is calculated by multiplying each feature by its corresponding weight, adding these up along with the bias, then applying the sigmoid function to produce a probability. The MSE is calculated as the average squared difference between each predicted probability and the corresponding true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4626]\n",
      " [0.4134]\n",
      " [0.6682]]\n",
      "0.3349\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn round(1/(1+np.exp(-x)),4)\n",
    "\n",
    "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
    "\t# Your code here\n",
    "\tfeatures = np.array(features)\n",
    "\tlabels = np.array(labels).reshape(-1,1)\n",
    "\tweights = np.array(weights).reshape(-1,1)\n",
    "\tlinear_output = features@weights+bias\n",
    "\tprobabilities = np.vectorize(sigmoid)(linear_output)\n",
    "\tmse = np.mean((probabilities-labels)**2).round(decimals=4)\n",
    "\treturn probabilities, mse\n",
    "\n",
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
    "labels = [0, 1, 0]\n",
    "weights = [0.7, -0.4]\n",
    "bias = -0.1\n",
    "probabilities, mse = single_neuron_model(features, labels, weights, bias)\n",
    "print(probabilities)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single neuron with backpropagation\n",
    "\n",
    "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n",
    "\n",
    "Example:\n",
    "\n",
    "        - input: features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2\n",
    "\n",
    "        - output: updated_weights = [0.0808, -0.1916], updated_bias = -0.0214, mse_values = [0.2386, 0.2348]\n",
    "        \n",
    "        - reasoning: The neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a single neuron (which is essentially a logistic regression model), the gradient of the loss function with respect to the weights and bias is calculated based on the chain rule of calculus. The specific structure you're using for the gradients is derived from how the neuron computes the error between predictions and true labels and how the neuron adjusts its parameters (weights and bias) accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1019], [-0.1711]]\n",
      "-0.0083\n",
      "[0.3033, 0.2987]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels).reshape(-1,1)\n",
    "    initial_weights = np.array(initial_weights).reshape(-1,1)\n",
    "    updated_weights = initial_weights.copy()\n",
    "    updated_bias = initial_bias\n",
    "    mse_values = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        linear_output = features@updated_weights+updated_bias\n",
    "        probabilities = sigmoid(linear_output)\n",
    "        errors = probabilities-labels #(-1,1)\n",
    "        mse_values.append(round(np.mean(errors**2), 4))\n",
    "        activation_grad = probabilities*(1-probabilities)\n",
    "        weights_grad = features.T@(errors*activation_grad)\n",
    "        bias_grad = np.sum(errors*activation_grad)\n",
    "        updated_weights -= learning_rate*weights_grad/len(labels)\n",
    "        updated_bias -= learning_rate*bias_grad/len(labels)\n",
    "\n",
    "    return np.round(updated_weights,4).tolist(), np.round(updated_bias,4), mse_values\n",
    "\n",
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n",
    "print(updated_weights)\n",
    "print(updated_bias)\n",
    "print(mse_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
