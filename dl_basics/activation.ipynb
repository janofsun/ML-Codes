{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU and Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Function\n",
    "\n",
    "Given a real-valued vector $( \\mathbf{z} = [z_1, z_2, \\dots, z_K] )$, the Softmax function converts it into a probability distribution $( \\mathbf{p} = [p_1, p_2, \\dots, p_K] )$, where each $( p_i )$ represents the probability corresponding to class $( i )$.\n",
    "\n",
    "The Softmax function is defined as:\n",
    "\n",
    "$$[\n",
    "p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } i = 1, 2, \\dots, K\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $( K )$ is the number of classes.\n",
    "- $( z_i )$ is the $( i )$-th component of the input vector.\n",
    "- $( p_i )$ is the probability corresponding to $( z_i )$.\n",
    "\n",
    "#### Explanation and Properties\n",
    "\n",
    "1. **Exponential Function**: The Softmax function first applies an exponential transformation to each input $( z_i )$, converting all input values into positive numbers. Since the exponential function is monotonic, larger input values will remain relatively larger after the exponential transformation, while smaller input values will become relatively smaller.\n",
    "\n",
    "2. **Normalization**: All exponentiated values are summed to create a normalization factor, and each exponentiated value is divided by this sum. This step ensures that the output probability distribution sums to 1.\n",
    "\n",
    "3. **Output Range**: The output of the Softmax function is a probability vector, where each element is bounded between 0 and 1. This makes Softmax well-suited for representing class probabilities in multi-class classification problems.\n",
    "\n",
    "4. **Class Competition**: A critical property of the Softmax function is its ability to amplify the differences between input values. If one input $( z_i )$ is significantly larger than the others, then $( p_i )$ will approach 1, while the other $( p_j )$ values will approach 0. This property makes Softmax effective for selecting the most likely class in multi-class classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def softmax(scores):\n",
    "\tprobabilities = []\n",
    "\tsum_exp_scores = sum([math.exp(score) for score in scores])\n",
    "\tfor score in scores:\n",
    "\t\tprobabilities.append(math.exp(score)/sum_exp_scores)\n",
    "\treturn probabilities\n",
    "\n",
    "scores = [1, 2, 3]\n",
    "# output: [0.0900, 0.2447, 0.6652]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Softmax\n",
    "In machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers.\n",
    "Given a real-valued vector $( \\mathbf{z} = [z_1, z_2, \\dots, z_K] )$, the Log Softmax is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LogSoftmax}(z_i) = \\log \\left( \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\right)\n",
    "$$\n",
    "\n",
    "This can be further simplified as:\n",
    "\n",
    "$$\n",
    "\\text{LogSoftmax}(z_i) = z_i - \\log \\left( \\sum_{j=1}^{K} e^{z_j} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_softmax(scores: list) -> np.ndarray:\n",
    "\t# Subtract the maximum value for numerical stability\n",
    "\tscores = scores - np.max(scores)\n",
    "\treturn scores - np.log(sum(np.exp(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw11785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
